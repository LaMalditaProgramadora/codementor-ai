# CodeMentor AI - Sistema de TutorÃ­a Inteligente

Sistema de evaluaciÃ³n automatizada de tareas de programaciÃ³n usando LLMs (Llama 3.1 70B), CodeBERT y Whisper.

## ğŸ“‹ DescripciÃ³n del Proyecto

CodeMentor AI es un Sistema de TutorÃ­a Inteligente (ITS) diseÃ±ado para automatizar la revisiÃ³n de proyectos de programaciÃ³n en cursos universitarios de IngenierÃ­a InformÃ¡tica. El sistema utiliza tecnologÃ­as de IA de vanguardia para proporcionar retroalimentaciÃ³n detallada y personalizada a los estudiantes.

### CaracterÃ­sticas Principales

âœ… **EvaluaciÃ³n AutomÃ¡tica de CÃ³digo**: AnÃ¡lisis profundo usando Llama 3.1 70B
âœ… **DetecciÃ³n de Plagio**: Similitud semÃ¡ntica y estructural con CodeBERT
âœ… **AnÃ¡lisis de Videos**: TranscripciÃ³n y anÃ¡lisis de presentaciones con Whisper
âœ… **Feedback Personalizado**: Comentarios constructivos por criterio de evaluaciÃ³n
âœ… **Portal Docente**: GestiÃ³n de tareas, secciones y calificaciones
âœ… **Portal Estudiante**: EnvÃ­o de proyectos y visualizaciÃ³n de feedback

## ğŸ—ï¸ Arquitectura del Sistema

### Stack TecnolÃ³gico

**Frontend:**
- React 18
- Tailwind CSS
- Vite

**Backend:**
- FastAPI
- SQLAlchemy
- Pydantic

**Base de Datos:**
- PostgreSQL 15 con pgvector

**Almacenamiento:**
- MinIO (S3-compatible)

**Servicios de IA:**
- **Ollama**: Llama 3.1 70B para evaluaciÃ³n de cÃ³digo
- **CodeBERT**: microsoft/codebert-base para detecciÃ³n de plagio
- **Whisper**: OpenAI Whisper para transcripciÃ³n de audio/video

**DevOps:**
- Docker & Docker Compose

### Diagrama de Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CAPA DE PRESENTACIÃ“N            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Portal     â”‚  â”‚   Portal     â”‚    â”‚
â”‚  â”‚  Estudiante  â”‚  â”‚   Docente    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTPS         â”‚ HTTPS
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CAPA DE LÃ“GICA                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚         â”‚   FastAPI    â”‚                â”‚
â”‚         â”‚   Backend    â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ALMACENAMIENTO   â”‚ â”‚  SERVICIOS IA    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Postgrâ”‚â”‚MinIO â”‚â”‚ â”‚ â”‚Ollama(Llama) â”‚ â”‚
â”‚ â”‚eSQL  â”‚â”‚      â”‚â”‚ â”‚ â”‚CodeBERT      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚ â”‚Whisper       â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- **Docker Desktop** 20.10+
- **Docker Compose** 2.0+
- **Git**
- **GPU NVIDIA** (recomendado para Ollama con modelo 70B)
  - Driver NVIDIA actualizado
  - NVIDIA Container Toolkit
- **MÃ­nimo 32GB RAM** (recomendado 64GB para Llama 3.1 70B)
- **MÃ­nimo 100GB espacio en disco**

### Paso 1: Clonar el Repositorio

```bash
git clone <repository-url>
cd codementor-ai
```

### Paso 2: Configurar Variables de Entorno

```bash
# En el directorio backend
cd backend
cp .env.example .env

# Editar .env si es necesario
nano .env
```

### Paso 3: Configurar NVIDIA Container Toolkit (Para GPU)

Si tienes GPU NVIDIA y quieres usar Llama 3.1 70B:

```bash
# Instalar NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### Paso 4: Iniciar Servicios con Docker Compose

```bash
# Desde el directorio raÃ­z del proyecto
docker-compose up -d
```

Esto iniciarÃ¡ todos los servicios:
- PostgreSQL (puerto 5432)
- MinIO (puertos 9000, 9001)
- Ollama (puerto 11434)
- Backend FastAPI (puerto 8000)
- Frontend React (puerto 5173)

### Paso 5: Verificar que los Servicios EstÃ©n Corriendo

```bash
# Ver logs de todos los servicios
docker-compose logs -f

# Ver logs de un servicio especÃ­fico
docker-compose logs -f backend
docker-compose logs -f ollama

# Ver estado de los contenedores
docker-compose ps
```

## ğŸ¤– ConfiguraciÃ³n de los Servicios de IA

### 1. Configurar Ollama con Llama 3.1 70B

Ollama debe descargar el modelo Llama 3.1 70B la primera vez. Este proceso puede tomar tiempo dependiendo de tu conexiÃ³n.

#### OpciÃ³n A: Descarga AutomÃ¡tica (Recomendado)

```bash
# Conectarse al contenedor de Ollama
docker exec -it codementor-ollama bash

# Descargar el modelo Llama 3.1 70B
ollama pull llama3.1:70b

# Verificar que el modelo estÃ¡ disponible
ollama list

# Salir del contenedor
exit
```

#### OpciÃ³n B: Usar un Modelo MÃ¡s PequeÃ±o (Para Testing o Recursos Limitados)

Si no tienes GPU o suficiente RAM, puedes usar un modelo mÃ¡s pequeÃ±o:

```bash
# En el contenedor de Ollama
docker exec -it codementor-ollama ollama pull llama3.1:8b

# Actualizar .env para usar el modelo mÃ¡s pequeÃ±o
# OLLAMA_MODEL=llama3.1:8b
```

#### Verificar que Ollama Funciona

```bash
# Probar el modelo
docker exec -it codementor-ollama ollama run llama3.1:70b "Hola, Â¿cÃ³mo estÃ¡s?"
```

### 2. Configurar CodeBERT

CodeBERT se descarga automÃ¡ticamente la primera vez que se usa. No requiere configuraciÃ³n adicional.

**VerificaciÃ³n:**

```bash
# Conectarse al contenedor del backend
docker exec -it codementor-backend bash

# Ejecutar Python y probar CodeBERT
python3 -c "
from app.services.codebert_service import codebert_service
codebert_service.initialize()
print('CodeBERT inicializado correctamente')
"

exit
```

### 3. Configurar Whisper

Whisper tambiÃ©n se descarga automÃ¡ticamente. Puedes elegir entre diferentes tamaÃ±os de modelo:

- `tiny`: MÃ¡s rÃ¡pido, menos preciso
- `base`: Balance (predeterminado)
- `small`: MÃ¡s preciso
- `medium`: Muy preciso
- `large`: MÃ¡xima precisiÃ³n (requiere mÃ¡s recursos)

**Cambiar el modelo de Whisper:**

Edita `.env`:
```
WHISPER_MODEL=small  # o tiny, base, medium, large
```

**VerificaciÃ³n:**

```bash
# En el contenedor del backend
docker exec -it codementor-backend python3 -c "
from app.services.whisper_service import whisper_service
whisper_service.initialize()
print('Whisper inicializado correctamente')
"
```

## ğŸ“Š Inicializar la Base de Datos

```bash
# Crear las tablas en PostgreSQL
docker exec -it codementor-backend python3 init_db.py

# Verificar que las tablas se crearon
docker exec -it codementor-postgres psql -U codementor_user -d codementor -c "\dt"
```

## ğŸŒ Acceder a las Interfaces

Una vez que todos los servicios estÃ©n corriendo:

- **Frontend (Portal Estudiante/Docente)**: http://localhost:5173
- **Backend API (Swagger Docs)**: http://localhost:8000/docs
- **MinIO Console**: http://localhost:9001
  - Usuario: `minioadmin`
  - ContraseÃ±a: `minioadmin123`
- **Ollama API**: http://ollama:11434

## ğŸ“ Uso del Sistema

### Para Docentes

1. **Crear una SecciÃ³n**
```bash
curl -X POST "http://localhost:8000/api/sections" \
  -H "Content-Type: application/json" \
  -d '{
    "section_id": "SEC001",
    "section_code": "CS101-A",
    "semester": "2025-1",
    "year": 2025,
    "instructor_id": 1
  }'
```

2. **Crear una Tarea**
```bash
curl -X POST "http://localhost:8000/api/assignments" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Proyecto Final - Sistema de GestiÃ³n",
    "description": "Desarrollar un sistema CRUD completo",
    "due_date": "2025-12-20T23:59:59",
    "max_score": 100,
    "requirements": "Debe incluir: frontend, backend, BD",
    "section_id": "SEC001"
  }'
```

### Para Estudiantes

1. **Enviar una Tarea**
```bash
curl -X POST "http://localhost:8000/api/submissions" \
  -F "assignment_id=1" \
  -F "section_id=SEC001" \
  -F "group_number=1" \
  -F "submitted_by=20190001" \
  -F "project_file=@proyecto.zip" \
  -F "video_file=@presentacion.mp4"
```

2. **Evaluar una Tarea**
```bash
curl -X POST "http://localhost:8000/api/submissions/1/evaluate"
```

### Ver Resultados

```bash
# Ver calificaciÃ³n
curl "http://localhost:8000/api/grades?submission_id=1"

# Ver feedback
curl "http://localhost:8000/api/submissions/1"
```

## ğŸ” DetecciÃ³n de Plagio

Para analizar plagio en todas las entregas de una tarea:

```bash
curl -X POST "http://localhost:8000/api/plagiarism/detect?assignment_id=1"
```

## ğŸ§ª Testing

```bash
# Ejecutar tests
docker exec -it codementor-backend pytest

# Con coverage
docker exec -it codementor-backend pytest --cov=app tests/
```

## ğŸ“ˆ Monitoreo y Logs

### Ver Logs en Tiempo Real

```bash
# Todos los servicios
docker-compose logs -f

# Solo el backend
docker-compose logs -f backend

# Solo Ollama
docker-compose logs -f ollama
```

### Ver Logs en la Base de Datos

```bash
# Conectarse a PostgreSQL
docker exec -it codementor-postgres psql -U codementor_user -d codementor

# Ver logs de evaluaciÃ³n
SELECT * FROM simple_logs ORDER BY timestamp DESC LIMIT 10;
```

## ğŸ› ï¸ Troubleshooting

### Problema: Ollama no puede descargar el modelo

**SoluciÃ³n:**
```bash
# Verificar conexiÃ³n a internet en el contenedor
docker exec -it codementor-ollama ping -c 4 google.com

# Intentar descargar manualmente
docker exec -it codementor-ollama ollama pull llama3.1:70b
```

### Problema: Out of Memory con Llama 3.1 70B

**SoluciÃ³n:**
```bash
# Usar un modelo mÃ¡s pequeÃ±o
docker exec -it codementor-ollama ollama pull llama3.1:8b

# Actualizar .env
OLLAMA_MODEL=llama3.1:8b

# Reiniciar backend
docker-compose restart backend
```

### Problema: CodeBERT no se descarga

**SoluciÃ³n:**
```bash
# Entrar al contenedor
docker exec -it codementor-backend bash

# Descargar manualmente
python3 -c "
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')
model = AutoModel.from_pretrained('microsoft/codebert-base')
print('CodeBERT descargado correctamente')
"
```

### Problema: Error de conexiÃ³n con MinIO

**SoluciÃ³n:**
```bash
# Verificar que MinIO estÃ¡ corriendo
docker-compose ps minio

# Reiniciar MinIO
docker-compose restart minio

# Verificar logs
docker-compose logs minio
```

## ğŸ“¦ Estructura del Proyecto

```
codementor-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚   â”‚       â”œâ”€â”€ submissions.py
â”‚   â”‚   â”‚       â”œâ”€â”€ assignments.py
â”‚   â”‚   â”‚       â”œâ”€â”€ grades.py
â”‚   â”‚   â”‚       â””â”€â”€ plagiarism.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ session.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ ollama_service.py
â”‚   â”‚       â”œâ”€â”€ codebert_service.py
â”‚   â”‚       â”œâ”€â”€ whisper_service.py
â”‚   â”‚       â”œâ”€â”€ minio_service.py
â”‚   â”‚       â””â”€â”€ evaluation_pipeline.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ init_db.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ”’ Consideraciones de Seguridad

- Cambiar credenciales por defecto en producciÃ³n
- Usar HTTPS en producciÃ³n
- Implementar autenticaciÃ³n y autorizaciÃ³n
- Configurar firewall para limitar acceso a puertos
- Mantener servicios actualizados

## ğŸ“š Referencias y DocumentaciÃ³n

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)

## ğŸ‘¥ CrÃ©ditos

Desarrollado como parte del proyecto de tesis:
"AutomatizaciÃ³n de la RevisiÃ³n de Tareas de ProgramaciÃ³n con un Sistema de TutorÃ­a Inteligente"

Universidad Nacional Mayor de San Marcos
Facultad de IngenierÃ­a de Sistemas e InformÃ¡tica

## ğŸ“„ Licencia

Este proyecto es parte de una tesis acadÃ©mica.

---

**Nota**: Este README cubre la implementaciÃ³n hasta la Semana 4 del cronograma, que incluye:
- âœ… Semana 1: Infraestructura (Docker, PostgreSQL, MinIO, Modelos)
- âœ… Semana 2: Backend API (FastAPI, endpoints, MinIO integration)
- âœ… Semana 3: IA - CÃ³digo (Ollama, CodeBERT, pipeline)
- âœ… Semana 4: IA - Video (Whisper, anÃ¡lisis, pipeline completo)
